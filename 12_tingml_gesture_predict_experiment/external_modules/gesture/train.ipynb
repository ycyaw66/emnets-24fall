{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 01 数据集导入，创建训练集和测试集。通过查看注释，理解整个流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1328, 60, 1) (332, 60, 1) (1328,) (332,)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "# 训练集、测试集划分\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 数据集相对路径\n",
    "DATA_PATH = \"../../../10_tingml_datasets/\"\n",
    "# LABELS 的内容尽量与前面store_data.py保持一致\n",
    "LABELS = [\"Stationary\", \"Tilted\", \"Rotating\", \"Moving\"]\n",
    "# 代表一个样本内容，如连续10次传感器读到的6轴数据作为一个样本\n",
    "SAMPLES_PER_GESTURE = 10\n",
    "def load_one_label_data(label):\n",
    "    path = DATA_PATH + label + \"*.npy\"\n",
    "    files = glob.glob(path)\n",
    "    datas = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            data = np.load(file)\n",
    "            # 切除多余数据，如数据当中有61份，但每个样本只需要10份，那么最后一份需要丢弃。\n",
    "            num_slice = len(data) // SAMPLES_PER_GESTURE\n",
    "            datas.append(data[: num_slice * SAMPLES_PER_GESTURE, :])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    datas = np.concatenate(datas, axis=0)\n",
    "    # 由于本案例给的是全连接层，输入为1维数据。(其余如conv需要自行根据模型输入修改尺寸，如二维)\n",
    "    # MLP\n",
    "    # datas = np.reshape(datas,(-1, 6 * SAMPLES_PER_GESTURE,),)  # Modified here\n",
    "    # CNN 1\n",
    "    datas = np.reshape(datas,(-1, 6 * SAMPLES_PER_GESTURE, 1),)  # Modified here\n",
    "    # CNN 2, height = SAMPLES_PER_GESTURE, width = 6\n",
    "    # datas = np.reshape(datas,(-1, SAMPLES_PER_GESTURE, 6, 1),) # Modified here\n",
    "    \n",
    "    idx = LABELS.index(label)\n",
    "    labels = np.ones(datas.shape[0]) * idx\n",
    "    return datas, labels\n",
    "all_datas = []\n",
    "all_labels = []\n",
    "# 导入每个label对应的数据\n",
    "for label in LABELS:\n",
    "    datas, labels = load_one_label_data(label)\n",
    "    all_datas.append(datas)\n",
    "    all_labels.append(labels)\n",
    "dataX = np.concatenate(all_datas, axis=0)\n",
    "dataY = np.concatenate(all_labels, axis=0)\n",
    "# 输入和样本到此创建完毕\n",
    "\n",
    "# 训练集、测试集划分\n",
    "# test_size 表示数据集里面有20%将划分给测试集\n",
    "# stratify=dataY指定按label进行划分, 确保数据集划分公平\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(\n",
    "    dataX, dataY, test_size=0.2, stratify=dataY\n",
    ")\n",
    "print(xTrain.shape, xTest.shape, yTrain.shape, yTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 02 模型创建\n",
    "下面将创建很简单的多层感知机模型，后续可自行定义模型结构。需要根据自身需求，自行上网查询其他模型，如CNN，切记模型不要太大，嵌入式设备大致提供32K空间供运行模型。\n",
    "模型需要注意输入尺寸，如CNN往往多维数据，如**Conv1d 输入二维，可将输入改为(6 * SAMPLES_PER_GESTURE,1)或者(SAMPLES_PER_GESTURE, 6), 上面数据集对应尺寸也需要修改**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_2 (Conv1D)           (None, 60, 8)             32        \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 60, 8)             200       \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 8)                 0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 340 (1.33 KB)\n",
      "Trainable params: 340 (1.33 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 0 = INFO, 1 = WARNING, 2 = ERROR, 3 = FATAL\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "SAMPLES_PER_GESTURE = 10\n",
    "LABELS = [\"Stationary\", \"Tilted\", \"Rotating\", \"Moving\"]\n",
    "\n",
    "# 设置环境变量，控制日志级别\n",
    "def mlp():\n",
    "    # 一个用于线性堆叠多个网络层的模型。\n",
    "    # Sequential模型是最简单的神经网络模型，它按照层的顺序依次堆叠，每一层的输出会成为下一层的输入。\n",
    "    model = keras.Sequential()\n",
    "    # 第一层, 添加全连接层，输出尺寸为64，激活函数采用\"relu\"\n",
    "    # 第一层需要制定输入大小，这里和数据集对应input_shape=(6 * SAMPLES_PER_GESTURE,)\n",
    "    model.add(keras.layers.Dense(64, activation=\"relu\", input_shape=(6 * SAMPLES_PER_GESTURE,)))\n",
    "    # 添加池化层，防止模型过拟合，每次自动忘记20%的参数\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    # 最后一层，全连接层，输出尺寸对应labels数量，激活函数采用\"softmax\"\n",
    "    # softmaxs输出的结果代表每个label的概率，如第0个代表label 0的概率\n",
    "    model.add(keras.layers.Dense(len(LABELS), activation=\"softmax\"))\n",
    "    return model\n",
    "def cnn():\n",
    "    # 一个用于线性堆叠多个网络层的模型。\n",
    "    # Sequential模型是最简单的神经网络模型，它按照层的顺序依次堆叠，每一层的输出会成为下一层的输入。\n",
    "    model = keras.Sequential()\n",
    "    # 注意CNN与MLP的输入shape\n",
    "    # 16个输出通道，3为卷积核大小\n",
    "    model.add(\n",
    "        keras.layers.Conv1D(\n",
    "            8,3,padding=\"same\",activation=\"relu\",input_shape=(6 * SAMPLES_PER_GESTURE, 1),\n",
    "        )\n",
    "    )\n",
    "    model.add(keras.layers.Conv1D(8, 3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(keras.layers.GlobalAveragePooling1D())\n",
    "    model.add(keras.layers.Dense(8, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(len(LABELS), activation=\"softmax\"))\n",
    "    return model\n",
    "def cnn_2d():\n",
    "    model = keras.Sequential()\n",
    "    model.add(\n",
    "        keras.layers.Conv2D(\n",
    "            filters=8,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            input_shape=(SAMPLES_PER_GESTURE, 6, 1),\n",
    "        )\n",
    "    )\n",
    "    model.add(keras.layers.Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(keras.layers.Dense(16, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(len(LABELS), activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "model = cnn()\n",
    "# 打印模型结构\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 03 模型训练及测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "140/166 [========================>.....] - ETA: 0s - loss: 1.6995 - sparse_categorical_accuracy: 0.2509    \n",
      "Epoch 1: val_sparse_categorical_accuracy improved from -inf to 0.39157, saving model to best_model.h5\n",
      "166/166 [==============================] - 1s 2ms/step - loss: 1.6095 - sparse_categorical_accuracy: 0.2658 - val_loss: 1.0675 - val_sparse_categorical_accuracy: 0.3916\n",
      "Epoch 2/50\n",
      "136/166 [=======================>......] - ETA: 0s - loss: 1.0828 - sparse_categorical_accuracy: 0.5175"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_sparse_categorical_accuracy improved from 0.39157 to 0.77108, saving model to best_model.h5\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1.0589 - sparse_categorical_accuracy: 0.5369 - val_loss: 0.9727 - val_sparse_categorical_accuracy: 0.7711\n",
      "Epoch 3/50\n",
      "165/166 [============================>.] - ETA: 0s - loss: 0.9752 - sparse_categorical_accuracy: 0.6545\n",
      "Epoch 3: val_sparse_categorical_accuracy did not improve from 0.77108\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.9752 - sparse_categorical_accuracy: 0.6559 - val_loss: 0.8864 - val_sparse_categorical_accuracy: 0.6506\n",
      "Epoch 4/50\n",
      "143/166 [========================>.....] - ETA: 0s - loss: 0.8618 - sparse_categorical_accuracy: 0.6556\n",
      "Epoch 4: val_sparse_categorical_accuracy did not improve from 0.77108\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.8598 - sparse_categorical_accuracy: 0.6498 - val_loss: 0.7918 - val_sparse_categorical_accuracy: 0.7199\n",
      "Epoch 5/50\n",
      "143/166 [========================>.....] - ETA: 0s - loss: 0.7852 - sparse_categorical_accuracy: 0.7045\n",
      "Epoch 5: val_sparse_categorical_accuracy improved from 0.77108 to 0.92771, saving model to best_model.h5\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.7720 - sparse_categorical_accuracy: 0.7108 - val_loss: 0.6918 - val_sparse_categorical_accuracy: 0.9277\n",
      "Epoch 6/50\n",
      "146/166 [=========================>....] - ETA: 0s - loss: 0.7003 - sparse_categorical_accuracy: 0.7423\n",
      "Epoch 6: val_sparse_categorical_accuracy improved from 0.92771 to 0.93072, saving model to best_model.h5\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.6810 - sparse_categorical_accuracy: 0.7568 - val_loss: 0.6003 - val_sparse_categorical_accuracy: 0.9307\n",
      "Epoch 7/50\n",
      "145/166 [=========================>....] - ETA: 0s - loss: 0.6110 - sparse_categorical_accuracy: 0.8000\n",
      "Epoch 7: val_sparse_categorical_accuracy improved from 0.93072 to 0.93675, saving model to best_model.h5\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.6056 - sparse_categorical_accuracy: 0.7982 - val_loss: 0.5324 - val_sparse_categorical_accuracy: 0.9367\n",
      "Epoch 8/50\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5425 - sparse_categorical_accuracy: 0.7974\n",
      "Epoch 8: val_sparse_categorical_accuracy did not improve from 0.93675\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.5425 - sparse_categorical_accuracy: 0.7974 - val_loss: 0.4846 - val_sparse_categorical_accuracy: 0.9337\n",
      "Epoch 9/50\n",
      "127/166 [=====================>........] - ETA: 0s - loss: 0.5077 - sparse_categorical_accuracy: 0.8307\n",
      "Epoch 9: val_sparse_categorical_accuracy did not improve from 0.93675\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.4913 - sparse_categorical_accuracy: 0.8373 - val_loss: 0.4217 - val_sparse_categorical_accuracy: 0.9187\n",
      "Epoch 10/50\n",
      "135/166 [=======================>......] - ETA: 0s - loss: 0.4626 - sparse_categorical_accuracy: 0.8287\n",
      "Epoch 10: val_sparse_categorical_accuracy did not improve from 0.93675\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.4558 - sparse_categorical_accuracy: 0.8358 - val_loss: 0.3889 - val_sparse_categorical_accuracy: 0.9277\n",
      "Epoch 11/50\n",
      "138/166 [=======================>......] - ETA: 0s - loss: 0.4310 - sparse_categorical_accuracy: 0.8379\n",
      "Epoch 11: val_sparse_categorical_accuracy did not improve from 0.93675\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.4208 - sparse_categorical_accuracy: 0.8464 - val_loss: 0.3338 - val_sparse_categorical_accuracy: 0.9277\n",
      "Epoch 12/50\n",
      "139/166 [========================>.....] - ETA: 0s - loss: 0.3873 - sparse_categorical_accuracy: 0.8651\n",
      "Epoch 12: val_sparse_categorical_accuracy improved from 0.93675 to 0.94277, saving model to best_model.h5\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.3894 - sparse_categorical_accuracy: 0.8607 - val_loss: 0.3006 - val_sparse_categorical_accuracy: 0.9428\n",
      "Epoch 13/50\n",
      "138/166 [=======================>......] - ETA: 0s - loss: 0.3657 - sparse_categorical_accuracy: 0.8641\n",
      "Epoch 13: val_sparse_categorical_accuracy did not improve from 0.94277\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.3573 - sparse_categorical_accuracy: 0.8682 - val_loss: 0.2650 - val_sparse_categorical_accuracy: 0.9307\n",
      "Epoch 14/50\n",
      "142/166 [========================>.....] - ETA: 0s - loss: 0.3230 - sparse_categorical_accuracy: 0.8952\n",
      "Epoch 14: val_sparse_categorical_accuracy did not improve from 0.94277\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.3180 - sparse_categorical_accuracy: 0.9006 - val_loss: 0.2408 - val_sparse_categorical_accuracy: 0.9337\n",
      "Epoch 15/50\n",
      "137/166 [=======================>......] - ETA: 0s - loss: 0.3129 - sparse_categorical_accuracy: 0.8942\n",
      "Epoch 15: val_sparse_categorical_accuracy improved from 0.94277 to 0.94578, saving model to best_model.h5\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.3084 - sparse_categorical_accuracy: 0.8938 - val_loss: 0.2209 - val_sparse_categorical_accuracy: 0.9458\n",
      "Epoch 16/50\n",
      "137/166 [=======================>......] - ETA: 0s - loss: 0.3024 - sparse_categorical_accuracy: 0.8960\n",
      "Epoch 16: val_sparse_categorical_accuracy did not improve from 0.94578\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.2956 - sparse_categorical_accuracy: 0.8976 - val_loss: 0.2019 - val_sparse_categorical_accuracy: 0.9458\n",
      "Epoch 17/50\n",
      "132/166 [======================>.......] - ETA: 0s - loss: 0.2779 - sparse_categorical_accuracy: 0.9072\n",
      "Epoch 17: val_sparse_categorical_accuracy improved from 0.94578 to 0.97289, saving model to best_model.h5\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.2764 - sparse_categorical_accuracy: 0.8983 - val_loss: 0.1772 - val_sparse_categorical_accuracy: 0.9729\n",
      "Epoch 18/50\n",
      "142/166 [========================>.....] - ETA: 0s - loss: 0.2593 - sparse_categorical_accuracy: 0.9217\n",
      "Epoch 18: val_sparse_categorical_accuracy improved from 0.97289 to 0.97590, saving model to best_model.h5\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.2614 - sparse_categorical_accuracy: 0.9187 - val_loss: 0.1616 - val_sparse_categorical_accuracy: 0.9759\n",
      "Epoch 19/50\n",
      "140/166 [========================>.....] - ETA: 0s - loss: 0.2524 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 19: val_sparse_categorical_accuracy did not improve from 0.97590\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.2549 - sparse_categorical_accuracy: 0.9217 - val_loss: 0.1601 - val_sparse_categorical_accuracy: 0.9759\n",
      "Epoch 20/50\n",
      "135/166 [=======================>......] - ETA: 0s - loss: 0.2371 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 20: val_sparse_categorical_accuracy did not improve from 0.97590\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.2468 - sparse_categorical_accuracy: 0.9292 - val_loss: 0.1420 - val_sparse_categorical_accuracy: 0.9759\n",
      "Epoch 21/50\n",
      "140/166 [========================>.....] - ETA: 0s - loss: 0.2413 - sparse_categorical_accuracy: 0.9196\n",
      "Epoch 21: val_sparse_categorical_accuracy did not improve from 0.97590\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.2383 - sparse_categorical_accuracy: 0.9239 - val_loss: 0.1417 - val_sparse_categorical_accuracy: 0.9759\n",
      "Epoch 22/50\n",
      "126/166 [=====================>........] - ETA: 0s - loss: 0.2316 - sparse_categorical_accuracy: 0.9276\n",
      "Epoch 22: val_sparse_categorical_accuracy improved from 0.97590 to 0.98795, saving model to best_model.h5\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 0.2331 - sparse_categorical_accuracy: 0.9239 - val_loss: 0.1255 - val_sparse_categorical_accuracy: 0.9880\n",
      "Epoch 23/50\n",
      "135/166 [=======================>......] - ETA: 0s - loss: 0.2028 - sparse_categorical_accuracy: 0.9444\n",
      "Epoch 23: val_sparse_categorical_accuracy improved from 0.98795 to 0.99096, saving model to best_model.h5\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.2106 - sparse_categorical_accuracy: 0.9428 - val_loss: 0.1148 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 24/50\n",
      "138/166 [=======================>......] - ETA: 0s - loss: 0.2116 - sparse_categorical_accuracy: 0.9420\n",
      "Epoch 24: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.2067 - sparse_categorical_accuracy: 0.9420 - val_loss: 0.1119 - val_sparse_categorical_accuracy: 0.9759\n",
      "Epoch 25/50\n",
      "136/166 [=======================>......] - ETA: 0s - loss: 0.2049 - sparse_categorical_accuracy: 0.9375\n",
      "Epoch 25: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.2171 - sparse_categorical_accuracy: 0.9360 - val_loss: 0.1093 - val_sparse_categorical_accuracy: 0.9880\n",
      "Epoch 26/50\n",
      "137/166 [=======================>......] - ETA: 0s - loss: 0.1843 - sparse_categorical_accuracy: 0.9562\n",
      "Epoch 26: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1831 - sparse_categorical_accuracy: 0.9563 - val_loss: 0.1004 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 27/50\n",
      "139/166 [========================>.....] - ETA: 0s - loss: 0.1897 - sparse_categorical_accuracy: 0.9478\n",
      "Epoch 27: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1833 - sparse_categorical_accuracy: 0.9495 - val_loss: 0.0958 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 28/50\n",
      "140/166 [========================>.....] - ETA: 0s - loss: 0.2186 - sparse_categorical_accuracy: 0.9366\n",
      "Epoch 28: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.2159 - sparse_categorical_accuracy: 0.9352 - val_loss: 0.0968 - val_sparse_categorical_accuracy: 0.9819\n",
      "Epoch 29/50\n",
      "141/166 [========================>.....] - ETA: 0s - loss: 0.1716 - sparse_categorical_accuracy: 0.9521\n",
      "Epoch 29: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1730 - sparse_categorical_accuracy: 0.9526 - val_loss: 0.0925 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 30/50\n",
      "142/166 [========================>.....] - ETA: 0s - loss: 0.1707 - sparse_categorical_accuracy: 0.9525\n",
      "Epoch 30: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1691 - sparse_categorical_accuracy: 0.9548 - val_loss: 0.0869 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 31/50\n",
      "141/166 [========================>.....] - ETA: 0s - loss: 0.1792 - sparse_categorical_accuracy: 0.9548\n",
      "Epoch 31: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1815 - sparse_categorical_accuracy: 0.9526 - val_loss: 0.0828 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 32/50\n",
      "137/166 [=======================>......] - ETA: 0s - loss: 0.1606 - sparse_categorical_accuracy: 0.9480\n",
      "Epoch 32: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1689 - sparse_categorical_accuracy: 0.9488 - val_loss: 0.0902 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 33/50\n",
      "135/166 [=======================>......] - ETA: 0s - loss: 0.1560 - sparse_categorical_accuracy: 0.9593\n",
      "Epoch 33: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1629 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.0755 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 34/50\n",
      "132/166 [======================>.......] - ETA: 0s - loss: 0.1718 - sparse_categorical_accuracy: 0.9564\n",
      "Epoch 34: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1724 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.0797 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 35/50\n",
      "132/166 [======================>.......] - ETA: 0s - loss: 0.1721 - sparse_categorical_accuracy: 0.9479\n",
      "Epoch 35: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1695 - sparse_categorical_accuracy: 0.9518 - val_loss: 0.0804 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 36/50\n",
      "135/166 [=======================>......] - ETA: 0s - loss: 0.1626 - sparse_categorical_accuracy: 0.9593\n",
      "Epoch 36: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1693 - sparse_categorical_accuracy: 0.9541 - val_loss: 0.0722 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 37/50\n",
      "138/166 [=======================>......] - ETA: 0s - loss: 0.1596 - sparse_categorical_accuracy: 0.9556\n",
      "Epoch 37: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1527 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.0696 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 38/50\n",
      "138/166 [=======================>......] - ETA: 0s - loss: 0.1722 - sparse_categorical_accuracy: 0.9384\n",
      "Epoch 38: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1746 - sparse_categorical_accuracy: 0.9413 - val_loss: 0.0687 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 39/50\n",
      "151/166 [==========================>...] - ETA: 0s - loss: 0.1626 - sparse_categorical_accuracy: 0.9487\n",
      "Epoch 39: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 0.1574 - sparse_categorical_accuracy: 0.9518 - val_loss: 0.0692 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 40/50\n",
      "132/166 [======================>.......] - ETA: 0s - loss: 0.1430 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 40: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1406 - sparse_categorical_accuracy: 0.9601 - val_loss: 0.0622 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 41/50\n",
      "142/166 [========================>.....] - ETA: 0s - loss: 0.1470 - sparse_categorical_accuracy: 0.9604\n",
      "Epoch 41: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1466 - sparse_categorical_accuracy: 0.9616 - val_loss: 0.0602 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 42/50\n",
      "140/166 [========================>.....] - ETA: 0s - loss: 0.1336 - sparse_categorical_accuracy: 0.9589\n",
      "Epoch 42: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1466 - sparse_categorical_accuracy: 0.9548 - val_loss: 0.0607 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 43/50\n",
      "134/166 [=======================>......] - ETA: 0s - loss: 0.1432 - sparse_categorical_accuracy: 0.9674\n",
      "Epoch 43: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1370 - sparse_categorical_accuracy: 0.9691 - val_loss: 0.0586 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 44/50\n",
      "137/166 [=======================>......] - ETA: 0s - loss: 0.1378 - sparse_categorical_accuracy: 0.9617\n",
      "Epoch 44: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1414 - sparse_categorical_accuracy: 0.9593 - val_loss: 0.0597 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 45/50\n",
      "141/166 [========================>.....] - ETA: 0s - loss: 0.1154 - sparse_categorical_accuracy: 0.9699\n",
      "Epoch 45: val_sparse_categorical_accuracy did not improve from 0.99096\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1224 - sparse_categorical_accuracy: 0.9646 - val_loss: 0.0597 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 46/50\n",
      "138/166 [=======================>......] - ETA: 0s - loss: 0.1364 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 46: val_sparse_categorical_accuracy improved from 0.99096 to 0.99398, saving model to best_model.h5\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1392 - sparse_categorical_accuracy: 0.9571 - val_loss: 0.0536 - val_sparse_categorical_accuracy: 0.9940\n",
      "Epoch 47/50\n",
      "139/166 [========================>.....] - ETA: 0s - loss: 0.1359 - sparse_categorical_accuracy: 0.9595\n",
      "Epoch 47: val_sparse_categorical_accuracy did not improve from 0.99398\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1354 - sparse_categorical_accuracy: 0.9586 - val_loss: 0.0529 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 48/50\n",
      "142/166 [========================>.....] - ETA: 0s - loss: 0.1373 - sparse_categorical_accuracy: 0.9577\n",
      "Epoch 48: val_sparse_categorical_accuracy did not improve from 0.99398\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1298 - sparse_categorical_accuracy: 0.9616 - val_loss: 0.0549 - val_sparse_categorical_accuracy: 0.9940\n",
      "Epoch 49/50\n",
      "137/166 [=======================>......] - ETA: 0s - loss: 0.1244 - sparse_categorical_accuracy: 0.9535\n",
      "Epoch 49: val_sparse_categorical_accuracy did not improve from 0.99398\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1262 - sparse_categorical_accuracy: 0.9548 - val_loss: 0.0510 - val_sparse_categorical_accuracy: 0.9910\n",
      "Epoch 50/50\n",
      "138/166 [=======================>......] - ETA: 0s - loss: 0.1153 - sparse_categorical_accuracy: 0.9647\n",
      "Epoch 50: val_sparse_categorical_accuracy did not improve from 0.99398\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.1144 - sparse_categorical_accuracy: 0.9631 - val_loss: 0.0543 - val_sparse_categorical_accuracy: 0.9910\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# 加载模型\n",
    "from tensorflow.keras.models import load_model\n",
    "# 测试模型性能\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# 模型训练优化器，学习率为0.001\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "# 制定模型优化器，和损失函数、评价指标\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "# 制定保存模型的路径\n",
    "filepath = \"best_model.h5\"\n",
    "model.save(filepath)\n",
    "# 训练时，保存最好模型\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath,\n",
    "    monitor=\"val_sparse_categorical_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode=\"max\",\n",
    ")\n",
    "# 模型训练, 训练集，batch_size为批大小，可提高训练速度\n",
    "# validation_data指明验证集，epochs表示训练迭代轮数\n",
    "# verbose=1表示打印训练日志\n",
    "# callbacks调用上述保存模型的方法\n",
    "history = model.fit(\n",
    "    xTrain,\n",
    "    yTrain,\n",
    "    batch_size=8,\n",
    "    validation_data=(xTest, yTest),\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint],\n",
    ")\n",
    "# 至此模型训练完毕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 961us/step\n",
      "[[ 68   0   0   0]\n",
      " [  0  84   0   0]\n",
      " [  0   0  76   0]\n",
      " [  2   0   0 102]]\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "model = load_model(filepath)\n",
    "# 模型推理，预测\n",
    "predictions = model.predict(xTest)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "# 查看混淆矩阵，效果越好，预测则集中在对角线。\n",
    "cm = confusion_matrix(yTest, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于运动状态简单，上述最简单的模型可能也会获得不错的性能，当运动状态变得复杂，上述MLP模型性能将很难满足需求。\n",
    "#### 04 生成最终部署的模型\n",
    "由于RIOT系统使用的时tflite-micro库且资源有限，将模型量化，并保存成tflite-micro可识别的格式，注意\n",
    "`data_test = np.reshape(data_test, (-1, 6 * SAMPLES_PER_GESTURE, ))`后的尺寸维度和大小需要和前面大致对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpp8yfzhed/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpp8yfzhed/assets\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpm3gof7uw/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpm3gof7uw/assets\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py:887: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "tensorflow/lite/kernels/conv.cc:344 input->dims->size != 4 (5 != 4)Node number 1 (CONV_2D) failed to prepare.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m converter\u001b[38;5;241m.\u001b[39mrepresentative_dataset \u001b[38;5;241m=\u001b[39m representative_data_gen\n\u001b[1;32m     35\u001b[0m converter\u001b[38;5;241m.\u001b[39moptimizations \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mOptimize\u001b[38;5;241m.\u001b[39mOPTIMIZE_FOR_SIZE]\n\u001b[0;32m---> 36\u001b[0m tflite_model \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite(tflite_model)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 量化前后对比\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py:1065\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(convert_func)\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1064\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1065\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_and_export_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py:1042\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_conversion_params_metric()\n\u001b[1;32m   1041\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[0;32m-> 1042\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m elapsed_time_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mprocess_time() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py:1526\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;129m@_export_metrics\u001b[39m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a keras model based on instance variables.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \n\u001b[1;32m   1517\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;124;03m      Invalid quantization parameters.\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1526\u001b[0m   saved_model_convert_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_as_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1527\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m saved_model_convert_result:\n\u001b[1;32m   1528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_convert_result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py:1507\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1503\u001b[0m   graph_def, input_tensors, output_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1504\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_keras_to_saved_model(temp_dir)\n\u001b[1;32m   1505\u001b[0m   )\n\u001b[1;32m   1506\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_model_dir:\n\u001b[0;32m-> 1507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTFLiteKerasModelConverterV2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_tensors\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1511\u001b[0m   shutil\u001b[38;5;241m.\u001b[39mrmtree(temp_dir, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py:1303\u001b[0m, in \u001b[0;36mTFLiteConverterBaseV2.convert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;66;03m# Converts model.\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m result \u001b[38;5;241m=\u001b[39m _convert_graphdef(\n\u001b[1;32m   1297\u001b[0m     input_data\u001b[38;5;241m=\u001b[39mgraph_def,\n\u001b[1;32m   1298\u001b[0m     input_tensors\u001b[38;5;241m=\u001b[39minput_tensors,\n\u001b[1;32m   1299\u001b[0m     output_tensors\u001b[38;5;241m=\u001b[39moutput_tensors,\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconverter_kwargs,\n\u001b[1;32m   1301\u001b[0m )\n\u001b[0;32m-> 1303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_tflite_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_quant_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_io\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental_new_quantizer\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    214\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n\u001b[0;32m--> 215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py:991\u001b[0m, in \u001b[0;36mTFLiteConverterBase._optimize_tflite_model\u001b[0;34m(self, model, quant_mode, quant_io)\u001b[0m\n\u001b[1;32m    989\u001b[0m   q_allow_float \u001b[38;5;241m=\u001b[39m quant_mode\u001b[38;5;241m.\u001b[39mis_allow_float()\n\u001b[1;32m    990\u001b[0m   q_variable_quantization \u001b[38;5;241m=\u001b[39m quant_mode\u001b[38;5;241m.\u001b[39menable_mlir_variable_quantization\n\u001b[0;32m--> 991\u001b[0m   model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_quantize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_in_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_out_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_activations_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_bias_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_allow_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_variable_quantization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m m_in_type \u001b[38;5;241m=\u001b[39m in_type \u001b[38;5;28;01mif\u001b[39;00m in_type \u001b[38;5;28;01melse\u001b[39;00m _dtypes\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m   1002\u001b[0m m_out_type \u001b[38;5;241m=\u001b[39m out_type \u001b[38;5;28;01mif\u001b[39;00m out_type \u001b[38;5;28;01melse\u001b[39;00m _dtypes\u001b[38;5;241m.\u001b[39mfloat32\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py:710\u001b[0m, in \u001b[0;36mTFLiteConverterBase._quantize\u001b[0;34m(self, result, input_type, output_type, activations_type, bias_type, allow_float, enable_variable_quantization)\u001b[0m\n\u001b[1;32m    706\u001b[0m calibrate_quantize \u001b[38;5;241m=\u001b[39m _calibrator\u001b[38;5;241m.\u001b[39mCalibrator(\n\u001b[1;32m    707\u001b[0m     result, custom_op_registerers_by_name, custom_op_registerers_by_func\n\u001b[1;32m    708\u001b[0m )\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_calibrate_only \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_new_quantizer:\n\u001b[0;32m--> 710\u001b[0m   calibrated \u001b[38;5;241m=\u001b[39m \u001b[43mcalibrate_quantize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalibrate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentative_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_gen\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_calibrate_only:\n\u001b[1;32m    715\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m calibrated\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    214\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n\u001b[0;32m--> 215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/optimize/calibrator.py:254\u001b[0m, in \u001b[0;36mCalibrator.calibrate\u001b[0;34m(self, dataset_gen)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;129m@convert_phase\u001b[39m(Component\u001b[38;5;241m.\u001b[39mOPTIMIZE_TFLITE_MODEL, SubComponent\u001b[38;5;241m.\u001b[39mCALIBRATE)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalibrate\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_gen):\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calibrates the model with specified generator.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    dataset_gen: A generator that generates calibration samples.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calibrator\u001b[38;5;241m.\u001b[39mCalibrate()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/optimize/calibrator.py:143\u001b[0m, in \u001b[0;36mCalibrator._feed_tensors\u001b[0;34m(self, dataset_gen, resize_input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calibrator\u001b[38;5;241m.\u001b[39mPrepare(\n\u001b[1;32m    140\u001b[0m         [\u001b[38;5;28mlist\u001b[39m(s\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m input_array], signature_key\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calibrator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPrepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_array\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m signature_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tensorflow/lite/kernels/conv.cc:344 input->dims->size != 4 (5 != 4)Node number 1 (CONV_2D) failed to prepare."
     ]
    }
   ],
   "source": [
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "# 加载模型\n",
    "model = load_model(filepath)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# 设置目标操作集，允许使用 TensorFlow 原生操作\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # 使用TFLite的内置操作\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS     # 允许使用部分TensorFlow的操作\n",
    "]\n",
    "\n",
    "# 禁用 tensor list ops 的低级优化\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "# 保存初始版本，后续对比用\n",
    "open(\"model_basic.tflite\", \"wb\").write(tflite_model)\n",
    "open(\"model.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "# 量化模型, 定义输入格式与大小，只需要修改(-1, 6 * SAMPLES_PER_GESTURE,)与上面对应即可，其余不用变\n",
    "data_test = xTest.astype(\"float32\")\n",
    "# np.reshape 和一开始数据集导入对应\n",
    "# MLP\n",
    "# data_test = np.reshape(data_test, (-1, 6 * SAMPLES_PER_GESTURE, ))\n",
    "# CNN 1\n",
    "# data_test = np.reshape(data_test, (-1, 6 * SAMPLES_PER_GESTURE, 1))\n",
    "# CNN 2\n",
    "data_test = np.reshape(data_test, (-1, SAMPLES_PER_GESTURE, 6, 1))\n",
    "data_ds = tf.data.Dataset.from_tensor_slices((data_test)).batch(1)\n",
    "\n",
    "# Rest of your code...\n",
    "def representative_data_gen():\n",
    "    for input_value in data_ds.take(100):\n",
    "        yield [input_value]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model = converter.convert()\n",
    "open(\"model.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "# 量化前后对比\n",
    "basic_model_size = os.path.getsize(\"model_basic.tflite\")\n",
    "print(\"Basic model is %d bytes\" % basic_model_size)\n",
    "quantized_model_size = os.path.getsize(\"model.tflite\")\n",
    "print(\"Quantized model is %d bytes\" % quantized_model_size)\n",
    "difference = basic_model_size - quantized_model_size\n",
    "print(\"Difference is %d bytes\" % difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面，通过量化，帮助我们模型节省了2440Bytes大小。需要验证量化后的模型输入格式和尺寸是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's verify the model on a few input digits\n",
    "# Instantiate an interpreter for the model\n",
    "model_quantized_reloaded = tf.lite.Interpreter(\"model.tflite\")\n",
    "\n",
    "# Allocate memory for each model\n",
    "model_quantized_reloaded.allocate_tensors()\n",
    "\n",
    "# Get the input and output tensors so we can feed in values and get the results\n",
    "model_quantized_input = model_quantized_reloaded.get_input_details()[0][\"index\"]\n",
    "model_quantized_output = model_quantized_reloaded.get_output_details()[0][\"index\"]\n",
    "# Create arrays to store the results\n",
    "model_quantized_predictions = np.empty(xTest.size)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in range(yTest.shape[0]):\n",
    "    # Reshape the data and ensure the type is float32\n",
    "    # test_data = np.reshape(\n",
    "    #     xTest[i],\n",
    "    #     (\n",
    "    #         1,\n",
    "    #         6 * SAMPLES_PER_GESTURE,\n",
    "    #         1,\n",
    "    #     ),\n",
    "    # ).astype(\"float32\")\n",
    "    test_data = np.expand_dims(xTest[i], axis=0).astype(\"float32\")\n",
    "    print(test_data.shape)\n",
    "    # Invoke the interpreter\n",
    "    model_quantized_reloaded.set_tensor(model_quantized_input, test_data)\n",
    "    model_quantized_reloaded.invoke()\n",
    "    model_quantized_prediction = model_quantized_reloaded.get_tensor(\n",
    "        model_quantized_output\n",
    "    )\n",
    "    result = np.argmax(model_quantized_prediction, axis=1)\n",
    "    if (result == yTest[i]):\n",
    "        count = count + 1\n",
    "    print(\"Digit: {} - Prediction:\\n{}\".format(yTest[i], model_quantized_prediction))\n",
    "    print(\"\")\n",
    "\n",
    "print(count / yTest.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
